{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from collections import namedtuple\n",
    "import gensim.utils\n",
    "from langdetect import detect\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ArticutAPI import Articut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "articut = articut_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkdata = pd.read_csv('../links.csv')\n",
    "titles = np.array(linkdata.title)\n",
    "data = np.array(linkdata.article)\n",
    "\n",
    "linkdata = linkdata[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhconv import convert\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans2tw(s):\n",
    "    try:\n",
    "        return convert(s, 'zh-tw')\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(s):\n",
    "    time.sleep(0.5)\n",
    "    res = articut.parse(s, level=\"lv2\", openDataPlaceAccessBOOL=True, wikiDataBOOL=True)\n",
    "    return res['result_segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkdata['article'] = linkdata['article'].apply(trans2cn)\n",
    "linkdata['title'] = linkdata['title'].apply(trans2cn)\n",
    "linkdata['token'] = linkdata['article'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLM/和/NSP/分別/對應/的/是/Token/級/別/和/句子/級/別/兩個/粒度/的/任務/，/可以/替代/NSP/的/任務/還/真/不少/，/粘/一些/任務/介紹/放到/這裡/（/懶/得/翻譯了/）/，/有/興趣/的/可以/參看/這篇/On/ /Losses/ /for/ /Modern/ /Language/ /Models/：/\\n/\\n/Token/級別/的/任務/：/\\n/\\n/1/./ Term/ /Frequency/ /prediction /(/TF/)/:/ Regression/ /predicting/ /a/ /token/’/s/ /frequency/ /in/ /the/ /rest/ /of/ /the/ /document/./ The/ /frequency/ /is/ re-/scaled/ /between 0 /and 10 /per/ /document/./\\n/\\n/2/./ Term/ Frequency-/Inverse/ /Document/ /Frequency/ /prediction /(/TF-/IDF/)/:/ Regression/ /predicting/ /a/ /token/’/s/ tf-/idf/ /that/ /has/ /been/ re-/scaled/ /between 0 /and 10 /per/ /document/./\\n/\\n/3/./ Sentence/ /Boundary/ /Objective /(/SBO/)/:/ Predict/ /the/ /masked/ /token/ /given/ /the/ /embeddings/ /of/ /the/ /adjacent/ /tokens/./\\n/\\n/4/./ Trigram-/Shuffling /(/TGS/)/:/ 6/-/way/ /classification/ /predicting/ /the/ /original/ /order/ /of/ /shuffled/ tri-/grams/./\\n/\\n/5/./ Token/ /Corruption/ /Prediction /(/TCP/)/:/ Binary/ /classification/ /of/ /whether/ /a/ /token/ /has/ /been/ /corrupted /(/inserted/,/ replaced/,/ permuted/)/ or/ /not/./\\n/\\n/6/./ Capitalization/ /Prediction /(/Cap/./)/:/ Binary/,/ whether/ /a/ /token/ /is/ /capitalized/ /or/ /not/./\\n/\\n/7/./ Token/ /Length/ /Prediction /(/TLP/)/:/ Regression/ /to/ /predict/ /the/ /length/ /of/ /the/ /WordPiece/ /token/./\\n/\\n/句子/級別/的/任務/：/\\n/\\n/8/./ Next/ /Sentence/ /Prediction /(/NSP/)/:/ Binary/,/ whether/ /the/ /second/ /sentence/ /follows/ /the/ /first/ /or/ /comes/ /from/ /a/ /separate/ /document/./\\n/\\n/9/./ Adjacent/ /Sentence/ /Prediction /(/ASP/)/:/ 3/-/way/ /classification/ /whether/ /the/ /second/ /sentence/ /proceeds/ /the/ /first/,/ precedes/ /the/ /first/,/ or/ /they/ /come/ /from/ /separate/ /documents/./\\n/\\n/10/./ Sentence/ /Ordering /(/SO/)/:/ Binary/,/ predicting/ /if/ /the/ /two/ /sentences/ /are/ /in/ /or/ /out/ /of/ /order/./\\n/\\n/11/./ Sentence/ /Distance/ /Prediction /(/SDP/)/:/ 3/-/way/ /classification/ /of/ /whether/ /the/ /second/ /sentence/ /proceeds/,/ the/ /two/ /sentences/ /are/ /noncontiguous/ /from/ /the/ /same/ /document/,/ or/ /come/ /from/ /separate/ /documents/./\\n/\\n/12/./ Sentence/ /Corruption/ /Prediction /(/SCP/)/:/ Binary/ /classification/ /of/ /whether/ /a/ /tokens/ /in/ /a/ /sentence/ /have/ /been/ /corrupted /(/inserted/,/ replaced/,/ permuted/)/ or/ /not/./\\n/\\n/13/./ Quick/ /Thoughts/ /variant /(/QT/)/:/ Split/ /each/ /batch/ /into/ /two/,/ where/ /the/ /second/ /half/ /contains/ /the/ /subsequent/ /sentences/ /of/ /the/ /first/ /half /(/e/./g/./ with/ /batch/ /size 32/,/ sentence 17 /follows/ /sentence 1/,/ sentence 18 /follows/ /sentence 2/,/./././)/./ We/ /use/ /an/ energy-/based/ /model/ /to/ /predict/ /the/ /correct/ /continuation/ /for/ /each/ /sentence/ /in/ /the/ /first/ /half/ /where/ /the/ /energy/ /between/ /two/ /sentences/ /is/ /defined/ /by/ /the/ /negative/ /cosine/ /similarity/ /of/ /their /[/CLS/]/ embeddings/./ We/ /use/ /one/ /model/ /to/ /encode/ /both/ /halves/ /concurrently/./ See/ /Figure 1/./\\n/\\n/14/./ FastSent/ /variant /(/FS/)/:/ Split/ /each/ /batch/ /into/ /two/,/ where/ /the/ /second/ /half/ /contains/ /the/ /subsequent/ /sentences/ /of/ /the/ /first/ /half /(/same/ /as/ /QT/ /above/)/./ The/ /loss/ /is/ /defined/ /as/ cross-/entropy/ /between 1.0/ and/ /the/ /cosine/ /similarity/ /of/ /a/ /sentence /[/CLS/]/ embedding/ /and/ /the/ /other/ /sentence/ /token/ /embeddings /(/[/CLS/]/ embedding/ /from/ /the/ /first/ /half/ /with/ /token/ /embeddings/ /from/ /the/ /second/ /half/ /and /[/CLS/]/ embeddings/ /from/ /second/ /half/ /with/ /token/ /embeddigns/ /from/ /the/ /first/ /half/)/./ We/ /use/ /one/ /model/ /to/ /encode/ /both/ /halves/ /concurrently/./\\n/\\n/p/./s/./ /關於/問題/描述/中/NSP/作用/不大/的/說法/，/以/往/一般/認為/因為/任務形式/太簡單/會/使/模型/關注/一些/淺顯/的/lexical/特徵/，/但/是/其實/也/有/文章/實驗/表明/在/特定/場景/（/例如/小規模/預/訓練/模型/）/下/，/BERT/ /style/（/MLM/+/NSP/）/的/預/訓練/結果/會/強於/RoBERTa/ /style/（/僅/MLM/）/：/\\n/\\n/所以/這/仍然/是/一個/有待/討論/的/觀點/。'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkdata['token'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags title original_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "alldoc = []\n",
    "\n",
    "regex = re.compile(\"[%s]\" % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/15 [00:00<00:00, 33.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对卷积的困惑\n",
      "\n",
      "卷积这个概念，很早以前就学过，但是一直没有搞懂。教科书上通常会给出定义，给出很多性质，也会用实例和图形进行解释，但究竟为什么要这么设计，这么计算，背后的意义是什么，往往语焉不详。作为一个学物理出身的人，一个公式倘若倘若给不出结合实际的直观的通俗的解释（也就是背后的“物理”意义），就觉得少了点什么，觉得不是真的懂了。\n",
      "\n",
      "教科书上一般定义函数 ​的卷积 ​如下：\n",
      "\n",
      "连续形式：​\n",
      "\n",
      "​​离散形式：​\n",
      "\n",
      "​并且也解释了，先对g函数进行翻转，相当于在数轴上把g函数从右边褶到左边去，也就是卷积的“卷”的由来。\n",
      "\n",
      "然后再把g函数平移到n，在这个位置对两个函数的对应点相乘，然后相加，这个过程是卷积的“积”的过程。\n",
      "\n",
      "这个只是从计算的方式上对公式进行了解释，从数学上讲无可挑剔，但进一步追问，为什么要先翻转再平移，这么设计有何用意？还是有点费解。\n",
      "\n",
      "在知乎，已经很多的热心网友对卷积举了很多形象的例子进行了解释，如卷地毯、丢骰子、打耳光、存钱等等。读完觉得非常生动有趣，但过细想想，还是感觉有些地方还是没解释清楚，甚至可能还有瑕疵，或者还可以改进（这些后面我会做一些分析）。\n",
      "\n",
      "带着问题想了两个晚上，终于觉得有些问题想通了，所以就写出来跟网友分享，共同学习提高。不对的地方欢迎评论拍砖。。。\n",
      "\n",
      "明确一下，这篇文章主要想解释两个问题：\n",
      "\n",
      "1. 卷积这个名词是怎么解释？“卷”是什么意思？“积”又是什么意思？\n",
      "\n",
      "2. 卷积背后的意义是什么，该如何解释？\n",
      "\n",
      "考虑的应用场景\n",
      "\n",
      "为了更好地理解这些问题，我们先给出两个典型的应用场景：\n",
      "\n",
      "1. 信号分析\n",
      "\n",
      "一个输入信号f(t)，经过一个线性系统（其特征可以用单位冲击响应函数g(t)描述）以后，输出信号应该是什么？实际上通过卷积运算就可以得到输出信号。\n",
      "\n",
      "2. 图像处理\n",
      "\n",
      "输入一幅图像f(x,y)，经过特定设计的卷积核g(x,y)进行卷积处理以后，输出图像将会得到模糊，边缘强化等各种效果。\n",
      "\n",
      "对卷积的理解\n",
      "\n",
      "对卷积这个名词的理解：所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。\n",
      "\n",
      "在连续情况下，叠加指的是对两个函数的乘积求积分，在离散情况下就是加权求和，为简单起见就统一称为叠加。\n",
      "\n",
      "整体看来是这么个过程：\n",
      "\n",
      "翻转——>滑动——>叠加——>滑动——>叠加——>滑动——>叠加.....\n",
      "\n",
      "多次滑动得到的一系列叠加值，构成了卷积函数。\n",
      "\n",
      "卷积的“卷”，指的的函数的翻转，从 g(t) 变成 g(-t) 的这个过程；同时，“卷”还有滑动的意味在里面（吸取了网友李文清的建议）。如果把卷积翻译为“褶积”，那么这个“褶”字就只有翻转的含义了。\n",
      "\n",
      "卷积的“积”，指的是积分/加权求和。\n",
      "\n",
      "有些文章只强调滑动叠加求和，而没有说函数的翻转，我觉得是不全面的；有的文章对“卷”的理解其实是“积”，我觉得是张冠李戴。\n",
      "\n",
      "对卷积的意义的理解：\n",
      "\n",
      "1. 从“积”的过程可以看到，我们得到的叠加值，是个全局的概念。以信号分析为例，卷积的结果是不仅跟当前时刻输入信号的响应值有关，也跟过去所有时刻输入信号的响应都有关系，考虑了对过去的所有输入的效果的累积。在图像处理的中，卷积处理的结果，其实就是把每个像素周边的，甚至是整个图像的像素都考虑进来，对当前像素进行某种加权处理。所以说，“积”是全局概念，或者说是一种“混合”，把两个函数在时间或者空间上进行混合。\n",
      "\n",
      "2. 那为什么要进行“卷”？直接相乘不好吗？我的理解，进行“卷”（翻转）的目的其实是施加一种约束，它指定了在“积”的时候以什么为参照。在信号分析的场景，它指定了在哪个特定时间点的前后进行“积”，在空间分析的场景，它指定了在哪个位置的周边进行累积处理。\n",
      "\n",
      "举例说明\n",
      "\n",
      "下面举几个例子说明为什么要翻转，以及叠加求和的意义。\n",
      "\n",
      "例1：信号分析\n",
      "\n",
      "如下图所示，输入信号是 f(t) ，是随时间变化的。系统响应函数是 g(t) ，图中的响应函数是随时间指数下降的，它的物理意义是说：如果在 t=0 的时刻有一个输入，那么随着时间的流逝，这个输入将不断衰减。换言之，到了 t=T时刻，原来在 t=0 时刻的输入f(0)的值将衰减为f(0)g(T)。\n",
      "\n",
      "​\n",
      "\n",
      "​​考虑到信号是连续输入的，也就是说，每个时刻都有新的信号进来，所以，最终输出的是所有之前输入信号的累积效果。如下图所示，在T=10时刻，输出结果跟图中带标记的区域整体有关。其中，f(10)因为是刚输入的，所以其输出结果应该是f(10)g(0)，而时刻t=9的输入f(9)，只经过了1个时间单位的衰减，所以产生的输出应该是 f(9)g(1)，如此类推，即图中虚线所描述的关系。这些对应点相乘然后累加，就是T=10时刻的输出信号值，这个结果也是f和g两个函数在T=10时刻的卷积值。\n",
      "\n",
      "​\n",
      "\n",
      "​​显然，上面的对应关系看上去比较难看，是拧着的，所以，我们把g函数对折一下，变成了g(-t)，这样就好看一些了。看到了吗？这就是为什么卷积要“卷”，要翻转的原因，这是从它的物理意义中给出的。\n",
      "\n",
      "​\n",
      "\n",
      "​​上图虽然没有拧着，已经顺过来了，但看上去还有点错位，所以再进一步平移T个单位，就是下图。它就是本文开始给出的卷积定义的一种图形的表述：\n",
      "\n",
      "​\n",
      "\n",
      "​​所以，在以上计算T时刻的卷积时，要维持的约束就是： t+ (T-t) = T 。这种约束的意义，大家可以自己体会。\n",
      "\n",
      "例2：丢骰子\n",
      "\n",
      "在本问题 如何通俗易懂地解释卷积？中排名第一的 马同学在中举了一个很好的例子（下面的一些图摘自马同学的文章，在此表示感谢），用丢骰子说明了卷积的应用。\n",
      "\n",
      "要解决的问题是：有两枚骰子，把它们都抛出去，两枚骰子点数加起来为4的概率是多少?\n",
      "\n",
      "​\n",
      "\n",
      "​​分析一下，两枚骰子点数加起来为4的情况有三种情况：1+3=4， 2+2=4, 3+1=4\n",
      "\n",
      "因此，两枚骰子点数加起来为4的概率为：\n",
      "\n",
      "​​写成卷积的方式就是：​\n",
      "\n",
      "​​在这里我想进一步用上面的翻转滑动叠加的逻辑进行解释。\n",
      "\n",
      "首先，因为两个骰子的点数和是4，为了满足这个约束条件，我们还是把函数 g 翻转一下，然后阴影区域上下对应的数相乘，然后累加，相当于求自变量为4的卷积值，如下图所示：\n",
      "\n",
      "​\n",
      "\n",
      "​​进一步，如此翻转以后，可以方便地进行推广去求两个骰子点数和为 n 时的概率，为f 和 g的卷积 f*g(n)，如下图所示：​\n",
      "\n",
      "​​由上图可以看到，函数 g 的滑动，带来的是点数和的增大。这个例子中对f和g的约束条件就是点数和，它也是卷积函数的自变量。有兴趣还可以算算，如果骰子的每个点数出现的概率是均等的，那么两个骰子的点数和n=7的时候，概率最大。\n",
      "\n",
      "例3：图像处理\n",
      "\n",
      "还是引用知乎问题 如何通俗易懂地解释卷积？中 马同学的例子。图像可以表示为矩阵形式（下图摘自马同学的文章）：\n",
      "\n",
      "​\n",
      "\n",
      "对图像的处理函数（如平滑，或者边缘提取），也可以用一个g矩阵来表示，如：\n",
      "\n",
      "注意，我们在处理平面空间的问题，已经是二维函数了，相当于：\n",
      "\n",
      "那么函数f和g的在（u，v）处的卷积 该如何计算呢？\n",
      "\n",
      "按卷积的定义，二维离散形式的卷积公式应该是：\n",
      "\n",
      "从卷积定义来看，应该是在x和y两个方向去累加（对应上面离散公式中的i和j两个下标），而且是无界的，从负无穷到正无穷。可是，真实世界都是有界的。例如，上面列举的图像处理函数g实际上是个3x3的矩阵，意味着，在除了原点附近以外，其它所有点的取值都为0。考虑到这个因素，上面的公式其实退化了，它只把坐标（u,v）附近的点选择出来做计算了。所以，真正的计算如下所示：\n",
      "\n",
      "​\n",
      "\n",
      "​​首先我们在原始图像矩阵中取出（u,v）处的矩阵：\n",
      "\n",
      "然后将图像处理矩阵翻转（这个翻转有点意思，可以有几种不同的理解，其效果是等效的：（1）先沿x轴翻转，再沿y轴翻转；（2）先沿x轴翻转，再沿y轴翻转；），如下：\n",
      "\n",
      "原始矩阵：\n",
      "\n",
      "翻转后的矩阵：\n",
      "\n",
      "（1）先沿x轴翻转，再沿y轴翻转\n",
      "\n",
      "（2）先沿y轴翻转，再沿x轴翻转\n",
      "\n",
      "计算卷积时，就可以用 和 的内积：\n",
      "\n",
      "请注意，以上公式有一个特点，做乘法的两个对应变量a,b的下标之和都是（u,v），其目的是对这种加权求和进行一种约束。这也是为什么要将矩阵g进行翻转的原因。以上矩阵下标之所以那么写，并且进行了翻转，是为了让大家更清楚地看到跟卷积的关系。这样做的好处是便于推广，也便于理解其物理意义。实际在计算的时候，都是用翻转以后的矩阵，直接求矩阵内积就可以了。\n",
      "\n",
      "以上计算的是（u,v）处的卷积，延x轴或者y轴滑动，就可以求出图像中各个位置的卷积，其输出结果是处理以后的图像（即经过平滑、边缘提取等各种处理的图像）。\n",
      "\n",
      "再深入思考一下，在算图像卷积的时候，我们是直接在原始图像矩阵中取了（u,v）处的矩阵，为什么要取这个位置的矩阵，本质上其实是为了满足以上的约束。因为我们要算（u，v）处的卷积，而g矩阵是3x3的矩阵，要满足下标跟这个3x3矩阵的和是（u,v），只能是取原始图像中以（u，v）为中心的这个3x3矩阵，即图中的阴影区域的矩阵。\n",
      "\n",
      "推而广之，如果如果g矩阵不是3x3，而是7x7，那我们就要在原始图像中取以（u，v）为中心的7x7矩阵进行计算。由此可见，这种卷积就是把原始图像中的相邻像素都考虑进来，进行混合。相邻的区域范围取决于g矩阵的维度，维度越大，涉及的周边像素越多。而矩阵的设计，则决定了这种混合输出的图像跟原始图像比，究竟是模糊了，还是更锐利了。\n",
      "\n",
      "比如说，如下图像处理矩阵将使得图像变得更为平滑，显得更模糊，因为它联合周边像素进行了平均处理：\n",
      "\n",
      "而如下图像处理矩阵将使得像素值变化明显的地方更为明显，强化边缘，而变化平缓的地方没有影响，达到提取边缘的目的：\n",
      "\n",
      "对一些解释的不同意见\n",
      "\n",
      "上面一些对卷积的形象解释，如知乎问题卷积为什么叫「卷」积？中 荆哲 ，以及问题 如何通俗易懂地解释卷积？中 马同学 等人提出的如下比喻：\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "​​其实图中“卷”的方向，是沿该方向进行积分求和的方向，并无翻转之意。因此，这种解释，并没有完整描述卷积的含义，对“卷”的理解值得商榷。\n",
      "\n",
      "一些参考资料\n",
      "\n",
      "《数字信号处理（第二版）》程乾生，北京大学出版社\n",
      "\n",
      "《信号与系统引论》 郑君里，应启珩，杨为理，高等教育出版社\n",
      "['led', 'by', 'woody', 'andys', 'toys', 'live', 'happily', 'in', 'his', 'room', 'until', 'andys', 'birthday', 'brings', 'buzz', 'lightyear', 'onto', 'the', 'scene', 'afraid', 'of', 'losing', 'his', 'place', 'in', 'andys', 'heart', 'woody', 'plots', 'against', 'buzz', 'but', 'when', 'circumstances', 'separate', 'buzz', 'and', 'woody', 'from', 'their', 'owner', 'the', 'duo', 'eventually', 'learns', 'to', 'put', 'aside', 'their', 'differences']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-4ee9b1c0b6ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#             words = tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#             tags = [n]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "for line_no, line in enumerate(tqdm(data)):\n",
    "    if(type(line) == str):\n",
    "        if (detect(line) == \"zh-cn\"):\n",
    "            # 清理文本\n",
    "            print(line)\n",
    "            # 斷詞\n",
    "            tokens = gensim.utils.to_unicode(line).lower()\n",
    "            print(token)\n",
    "            raise\n",
    "#             words = tokens\n",
    "#             tags = [n]\n",
    "#             title = titles[line_no]\n",
    "#             alldoc.append(SentimentDocument(words, tags, title, line_no))\n",
    "#             n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45466/45466 [01:52<00:00, 405.73it/s]\n"
     ]
    }
   ],
   "source": [
    "for line_no, line in enumerate(tqdm(data)):\n",
    "    if (type(line) == str):\n",
    "        if(len(line) > 100):\n",
    "            if (detect(line) == \"en\"):\n",
    "                # 清理文本\n",
    "                line = regex.sub('', line)\n",
    "                # 斷詞\n",
    "                tokens = gensim.utils.to_unicode(line).lower().split()\n",
    "                words = tokens\n",
    "                tags = [n]\n",
    "                title = titles[line_no]\n",
    "                alldoc.append(SentimentDocument(words, tags, title, line_no))\n",
    "                n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldocs = alldoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents :  82205\n",
      "Mean length of documents :  59.03341645885287\n"
     ]
    }
   ],
   "source": [
    "l = []\n",
    "for doc in alldocs:\n",
    "    l.append(len(doc.words))\n",
    "\n",
    "print('Number of Documents : ', len(alldocs))\n",
    "print('Mean length of documents : ', np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentDocument(words=['a', 'mentally', 'unstable', 'vietnam', 'war', 'veteran', 'works', 'as', 'a', 'nighttime', 'taxi', 'driver', 'in', 'new', 'york', 'city', 'where', 'the', 'perceived', 'decadence', 'and', 'sleaze', 'feeds', 'his', 'urge', 'for', 'violent', 'action', 'attempting', 'to', 'save', 'a', 'preadolescent', 'prostitute', 'in', 'the', 'process'], tags=[100], title='Taxi Driver', original_number=109) \n",
      "\n",
      "A mentally unstable Vietnam War veteran works as a night-time taxi driver in New York City where the perceived decadence and sleaze feeds his urge for violent action, attempting to save a preadolescent prostitute in the process.\n"
     ]
    }
   ],
   "source": [
    "index = 100\n",
    "doc = alldocs[index]\n",
    "print(doc, '\\n')\n",
    "print(data[doc.original_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PV-DM\n",
    "model = Doc2Vec(dm=1, vector_size=300, window=10,hs=0,min_count=10,dbow_words=1,sample=1e-5)\n",
    "\n",
    "# build the vocabulary \n",
    "model.build_vocab(alldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(alldocs, total_examples=model.corpus_count, epochs=100, start_alpha=0.01, end_alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"movie_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('outer', 0.7869816422462463),\n",
       " ('astronauts', 0.7510691285133362),\n",
       " ('shuttle', 0.7495653033256531),\n",
       " ('manned', 0.7483032941818237),\n",
       " ('nasa', 0.7482559680938721),\n",
       " ('earth', 0.7396352291107178),\n",
       " ('hubble', 0.7361984848976135),\n",
       " ('spacecraft', 0.7320917844772339),\n",
       " ('planet', 0.7261258959770203),\n",
       " ('astronaut', 0.7245139479637146)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar_cosmul(positive = [\"space\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluating doc embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document :  Based on Charles Perrault's fairy tale: Cinderella is mistreated by her stepmother and stepsisters, but she is able to go to the Royal Ball with the help of the Fairy Godmother. \n",
      "\n",
      "Titre :  Cinderella\n",
      "Distance :  0.735744059085846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labpc1/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Closest document to the word \"war\"\n",
    "\n",
    "tokens = \"frog princess\"\n",
    "\n",
    "new_vector = model.infer_vector(tokens.split() ,alpha=0.001 ,steps = 5)\n",
    "tagsim = model.docvecs.most_similar([new_vector])[0]\n",
    "\n",
    "docsim = alldocs[tagsim[0] ]\n",
    "\n",
    "print(\"Document : \", data[docsim.original_number], \"\\n\")\n",
    "print(\"Titre : \", docsim.title)\n",
    "print(\"Distance : \", tagsim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target :  A struggling female soprano finds work playing a male female impersonator, but it complicates her personal life. \n",
      "\n",
      "Most :  A struggling female soprano finds work playing a male female impersonator, but it complicates her personal life. \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/labpc1/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0b2a74034ec1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moriginal_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Most : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Median : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m34000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Least : \"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malldocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_number\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "doc = np.random.randint(1000)\n",
    "\n",
    "sims = model.docvecs.most_similar(doc, topn=50) # get *all* similar documents\n",
    "\n",
    "print(\"Target : \", data[alldocs[doc] .original_number], \"\\n\" )\n",
    "print(\"Most : \" , data[alldocs[sims[0][0]].original_number], \"\\n\")\n",
    "print(\"Median : \" , data[alldocs[sims[34000][0]].original_number], \"\\n\")\n",
    "print(\"Least : \" , data[alldocs[sims[-1][0]].original_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
